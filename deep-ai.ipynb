{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import itertools as it\n",
    "import cv2\n",
    "from vizdoom import *\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Doom-Ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_game():\n",
    "    game = DoomGame()\n",
    "    game.set_doom_scenario_path(\"/home/msi-gtfo/repos/ViZDoom/scenarios/basic.wad\")\n",
    "    game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "    game.set_screen_format(ScreenFormat.RGB24)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)\n",
    "    game.set_render_particles(False)\n",
    "    game.set_window_visible(False)\n",
    "\n",
    "    # Available Buttons\n",
    "    game.add_available_button(Button.MOVE_LEFT)\n",
    "    game.add_available_button(Button.MOVE_RIGHT)\n",
    "    game.add_available_button(Button.ATTACK)\n",
    "\n",
    "\n",
    "    game.set_living_reward(-1)\n",
    "    game.init()\n",
    "\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Q-Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REPLAY_MEMORY = 1000\n",
    "MAX_REPLAY_MEMORY = 5000\n",
    "MINI_BATCH_SIZE   = 64\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "\n",
    "resolution = (30,45)\n",
    "frame_repeat = 12\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.resize(img, (resolution[1],resolution[0]))\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "def create_model(n_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128,3,input_shape=(resolution[0], resolution[1], 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Conv2D(128,3, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_actions))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(opt,'mse',['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, n_actions):\n",
    "        self.model = create_model(n_actions)\n",
    "        self.target_model = create_model(n_actions)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=MAX_REPLAY_MEMORY)\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        tmp = np.array(state)\n",
    "        tmp = tmp / 255\n",
    "        tmp = np.expand_dims(tmp, axis=0)\n",
    "        prediction = self.model.predict(tmp)\n",
    "        return prediction[0]\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINI_BATCH_SIZE)\n",
    "\n",
    "        current_states = np.array([transition[0] for transition in minibatch]) / 255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch]) / 255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X.append(current_state)\n",
    "            Y.append(current_qs)\n",
    "\n",
    "        self.model.fit(np.array(X)/255, np.array(Y), batch_size=MINI_BATCH_SIZE, verbose=0, shuffle=False if terminal_state else None)\n",
    "\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Starting training\nEpisode  0\nNot enough replay data:  1\nNot enough replay data:  2\nNot enough replay data:  3\nNot enough replay data:  4\nNot enough replay data:  5\nNot enough replay data:  6\nNot enough replay data:  7\nNot enough replay data:  8\nNot enough replay data:  9\nNot enough replay data:  10\nNot enough replay data:  11\nNot enough replay data:  12\nNot enough replay data:  13\nNot enough replay data:  14\nNot enough replay data:  15\nNot enough replay data:  16\nNot enough replay data:  17\nNot enough replay data:  18\nNot enough replay data:  19\nNot enough replay data:  20\nNot enough replay data:  21\nNot enough replay data:  22\nNot enough replay data:  23\nNot enough replay data:  24\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'screen_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c4e773ca380c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0misterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misterminal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'screen_buffer'"
     ]
    }
   ],
   "source": [
    "actions = [list(a) for a in it.product([0, 1], repeat=3)]\n",
    "epochs = 15\n",
    "\n",
    "agent = DQNAgent(len(actions))\n",
    "game = configure_game()\n",
    "\n",
    "scores = []\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"Episode \",epoch)\n",
    "    game.new_episode()\n",
    "   \n",
    "    while not game.is_episode_finished():\n",
    "        s1 = preprocess(game.get_state().screen_buffer)\n",
    "\n",
    "        if random.random() <= 0.2:\n",
    "            a = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            a = np.argmax(agent.get_qs(s1))\n",
    "\n",
    "        reward = game.make_action(actions[a], frame_repeat)\n",
    "        isterminal = game.is_episode_finished()\n",
    "\n",
    "        s2 = preprocess(game.get_state().screen_buffer) if not isterminal else [-1]\n",
    "        \n",
    "        agent.update_replay_memory([s1, a, reward, s2, isterminal])      \n",
    "        agent.train(isterminal)\n",
    "\n",
    "    if game.is_episode_finished():\n",
    "        final_reward = game.get_total_reward()\n",
    "        print(\"Final Reward: \", final_reward)\n",
    "        scores.append(final_reward)\n",
    "\n",
    "train_scores = np.array(scores)\n",
    "\n",
    "print(\"Results: mean: %.1fÂ±%.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "         \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitce553f8d58a746c5874dcf1bfcd11db3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import itertools as it\n",
    "import cv2\n",
    "from vizdoom import *   \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Doom-Ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_game_training():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"/home/msi-gtfo/repos/ViZDoom/scenarios/basic.cfg\")\n",
    "    game.set_window_visible(False)\n",
    "    #game.set_render_hud(False)\n",
    "    game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "    nothing     = [0, 0, 0]\n",
    "    left        = [1, 0, 0]\n",
    "    right       = [0, 1, 0]\n",
    "    shoot       = [0, 0, 1]\n",
    "    left_shoot  = [1, 0, 1]\n",
    "    right_shoot = [0, 1, 1]\n",
    "    possible_actions = [nothing, left, right, shoot, left_shoot, right_shoot]\n",
    "\n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Q-Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REPLAY_MEMORY = 1000\n",
    "MAX_REPLAY_MEMORY = 50000\n",
    "MINI_BATCH_SIZE   = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.2\n",
    "EPSILON_DISCOUNT =0.01\n",
    "\n",
    "resolution = (84,84)\n",
    "\n",
    "def preprocess(img):\n",
    "    #img = np.reshape(img,(img.shape[1],img.shape[2],img.shape[0]))\n",
    "    img = cv2.resize(img, (resolution[1],resolution[0]))\n",
    "    img = img.astype(np.float32)\n",
    "    img = img / 255\n",
    "    return img\n",
    "\n",
    "def create_model(n_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8,6,input_shape=(4, resolution[0], resolution[1]), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(8,6, activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(n_actions))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(opt,'mean_squared_error',['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, n_actions, use_latest=False):\n",
    "        self.model = create_model(n_actions)\n",
    "        self.replay_memory = deque(maxlen=MAX_REPLAY_MEMORY)\n",
    "        if use_latest:\n",
    "            self.load_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save(\"model\")\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = K.models.load_model(\"model\")\n",
    "\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, states):\n",
    "        states_exp = np.expand_dims(states, axis=0)\n",
    "        prediction = self.model.predict(states_exp)\n",
    "        return prediction[0]\n",
    "\n",
    "    # Trains network every step during episode\n",
    "    def train(self):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINI_BATCH_SIZE)\n",
    "\n",
    "        s1_batch = np.array([d[0] for d in minibatch])\n",
    "        a_batch = [d[1] for d in minibatch]\n",
    "        r_batch = [d[2] for d in minibatch]\n",
    "        s2_batch = np.array([d[3] for d in minibatch])\n",
    "\n",
    "        Y = []\n",
    "        s2_qs = self.model.predict(s2_batch)\n",
    "\n",
    "        for i in range(0,MINI_BATCH_SIZE):\n",
    "            value = 0\n",
    "            # Check if terminal\n",
    "            if minibatch[i][4]:\n",
    "                value = r_batch[i]\n",
    "            else:\n",
    "                value = r_batch[i] + GAMMA * np.max(s2_qs[i])\n",
    "            tmp = np.zeros(len(actions))\n",
    "            tmp[minibatch[i][1]] = value\n",
    "            Y.append(tmp)\n",
    "        \n",
    "        self.model.fit(s1_batch, np.array(Y), batch_size=MINI_BATCH_SIZE, epochs=1, verbose=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------Starting training Doom Ai--------------\n-> Episode  0\nFinal Reward:  57.0\nWARNING:tensorflow:From /home/msi-gtfo/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  1\nFinal Reward:  0.0\n-> Episode  2\nFinal Reward:  76.0\n-> Episode  3\nFinal Reward:  -405.0\n-> Episode  4\nFinal Reward:  -400.0\n-> Episode  5\nFinal Reward:  -410.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  6\nFinal Reward:  -395.0\n-> Episode  7\nFinal Reward:  95.0\n-> Episode  8\nFinal Reward:  -405.0\n-> Episode  9\nFinal Reward:  0.0\n-> Episode  10\nFinal Reward:  -320.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  11\nFinal Reward:  25.0\n-> Episode  12\nFinal Reward:  -335.0\n-> Episode  13\nFinal Reward:  -310.0\n-> Episode  14\nFinal Reward:  -188.0\n-> Episode  15\nFinal Reward:  -360.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  16\nFinal Reward:  -370.0\n-> Episode  17\nFinal Reward:  76.0\n-> Episode  18\nFinal Reward:  -355.0\n-> Episode  19\nFinal Reward:  95.0\n-> Episode  20\nFinal Reward:  -197.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  21\nFinal Reward:  71.0\n-> Episode  22\nFinal Reward:  95.0\n-> Episode  23\nFinal Reward:  -69.0\n-> Episode  24\nFinal Reward:  -40.0\n-> Episode  25\nFinal Reward:  -170.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  26\nFinal Reward:  -375.0\n-> Episode  27\nFinal Reward:  6.0\n-> Episode  28\nFinal Reward:  37.0\n-> Episode  29\nFinal Reward:  -247.0\n-> Episode  30\nFinal Reward:  -33.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  31\nFinal Reward:  95.0\n-> Episode  32\nFinal Reward:  95.0\n-> Episode  33\nFinal Reward:  83.0\n-> Episode  34\nFinal Reward:  -119.0\n-> Episode  35\nFinal Reward:  37.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  36\nFinal Reward:  64.0\n-> Episode  37\nFinal Reward:  14.0\n-> Episode  38\nFinal Reward:  -360.0\n-> Episode  39\nFinal Reward:  83.0\n-> Episode  40\nFinal Reward:  95.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  41\nFinal Reward:  -49.0\n-> Episode  42\nFinal Reward:  47.0\n-> Episode  43\nFinal Reward:  -11.0\n-> Episode  44\nFinal Reward:  64.0\n-> Episode  45\nFinal Reward:  -365.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  46\nFinal Reward:  95.0\n-> Episode  47\nFinal Reward:  83.0\n-> Episode  48\nFinal Reward:  -365.0\n-> Episode  49\nFinal Reward:  -350.0\n-> Episode  50\nFinal Reward:  -25.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  51\nFinal Reward:  83.0\n-> Episode  52\nFinal Reward:  76.0\n-> Episode  53\nFinal Reward:  -261.0\n-> Episode  54\nFinal Reward:  64.0\n-> Episode  55\nFinal Reward:  37.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  56\nFinal Reward:  -365.0\n-> Episode  57\nFinal Reward:  -335.0\n-> Episode  58\nFinal Reward:  45.0\n-> Episode  59\nFinal Reward:  95.0\n-> Episode  60\nFinal Reward:  -127.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  61\nFinal Reward:  -375.0\n-> Episode  62\nFinal Reward:  -390.0\n-> Episode  63\nFinal Reward:  -148.0\n-> Episode  64\nFinal Reward:  95.0\n-> Episode  65\nFinal Reward:  8.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  66\nFinal Reward:  -11.0\n-> Episode  67\nFinal Reward:  28.0\n-> Episode  68\nFinal Reward:  95.0\n-> Episode  69\nFinal Reward:  95.0\n-> Episode  70\nFinal Reward:  83.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  71\nFinal Reward:  64.0\n-> Episode  72\nFinal Reward:  -179.0\n-> Episode  73\nFinal Reward:  -370.0\n-> Episode  74\nFinal Reward:  83.0\n-> Episode  75\nFinal Reward:  -64.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  76\nFinal Reward:  95.0\n-> Episode  77\nFinal Reward:  6.0\n-> Episode  78\nFinal Reward:  47.0\n-> Episode  79\nFinal Reward:  -57.0\n-> Episode  80\nFinal Reward:  95.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  81\nFinal Reward:  -355.0\n-> Episode  82\nFinal Reward:  -98.0\n-> Episode  83\nFinal Reward:  -139.0\n-> Episode  84\nFinal Reward:  25.0\n-> Episode  85\nFinal Reward:  66.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  86\nFinal Reward:  95.0\n-> Episode  87\nFinal Reward:  -345.0\n-> Episode  88\nFinal Reward:  95.0\n-> Episode  89\nFinal Reward:  -212.0\n-> Episode  90\nFinal Reward:  -160.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  91\nFinal Reward:  83.0\n-> Episode  92\nFinal Reward:  76.0\n-> Episode  93\nFinal Reward:  -83.0\n-> Episode  94\nFinal Reward:  38.0\n-> Episode  95\nFinal Reward:  95.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  96\nFinal Reward:  30.0\n-> Episode  97\nFinal Reward:  95.0\n-> Episode  98\nFinal Reward:  -63.0\n-> Episode  99\nFinal Reward:  0.0\n-> Episode  100\nFinal Reward:  71.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  101\nFinal Reward:  -11.0\n-> Episode  102\nFinal Reward:  -39.0\n-> Episode  103\nFinal Reward:  -380.0\n-> Episode  104\nFinal Reward:  95.0\n-> Episode  105\nFinal Reward:  -62.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  106\nFinal Reward:  83.0\n-> Episode  107\nFinal Reward:  59.0\n-> Episode  108\nFinal Reward:  -330.0\n-> Episode  109\nFinal Reward:  95.0\n-> Episode  110\nFinal Reward:  -258.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  111\nFinal Reward:  76.0\n-> Episode  112\nFinal Reward:  95.0\n-> Episode  113\nFinal Reward:  -166.0\n-> Episode  114\nFinal Reward:  -41.0\n-> Episode  115\nFinal Reward:  95.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  116\nFinal Reward:  95.0\n-> Episode  117\nFinal Reward:  -390.0\n-> Episode  118\nFinal Reward:  57.0\n-> Episode  119\nFinal Reward:  76.0\n-> Episode  120\nFinal Reward:  52.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  121\nFinal Reward:  95.0\n-> Episode  122\nFinal Reward:  25.0\n-> Episode  123\nFinal Reward:  -141.0\n-> Episode  124\nFinal Reward:  26.0\n-> Episode  125\nFinal Reward:  -340.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  126\nFinal Reward:  -81.0\n-> Episode  127\nFinal Reward:  -360.0\n-> Episode  128\nFinal Reward:  95.0\n-> Episode  129\nFinal Reward:  6.0\n-> Episode  130\nFinal Reward:  83.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  131\nFinal Reward:  -375.0\n-> Episode  132\nFinal Reward:  -52.0\n-> Episode  133\nFinal Reward:  -380.0\n-> Episode  134\nFinal Reward:  -14.0\n-> Episode  135\nFinal Reward:  -176.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  136\nFinal Reward:  83.0\n-> Episode  137\nFinal Reward:  -360.0\n-> Episode  138\nFinal Reward:  83.0\n-> Episode  139\nFinal Reward:  76.0\n-> Episode  140\nFinal Reward:  57.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  141\nFinal Reward:  -365.0\n-> Episode  142\nFinal Reward:  -82.0\n-> Episode  143\nFinal Reward:  83.0\n-> Episode  144\nFinal Reward:  -170.0\n-> Episode  145\nFinal Reward:  13.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  146\nFinal Reward:  38.0\n-> Episode  147\nFinal Reward:  -49.0\n-> Episode  148\nFinal Reward:  -50.0\n-> Episode  149\nFinal Reward:  -5.0\n-> Episode  150\nFinal Reward:  76.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  151\nFinal Reward:  -134.0\n-> Episode  152\nFinal Reward:  45.0\n-> Episode  153\nFinal Reward:  -3.0\n-> Episode  154\nFinal Reward:  -375.0\n-> Episode  155\nFinal Reward:  13.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  156\nFinal Reward:  -109.0\n-> Episode  157\nFinal Reward:  66.0\n-> Episode  158\nFinal Reward:  30.0\n-> Episode  159\nFinal Reward:  25.0\n-> Episode  160\nFinal Reward:  83.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  161\nFinal Reward:  28.0\n-> Episode  162\nFinal Reward:  -340.0\n-> Episode  163\nFinal Reward:  0.0\n-> Episode  164\nFinal Reward:  76.0\n-> Episode  165\nFinal Reward:  -370.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  166\nFinal Reward:  83.0\n-> Episode  167\nFinal Reward:  95.0\n-> Episode  168\nFinal Reward:  -206.0\n-> Episode  169\nFinal Reward:  -355.0\n-> Episode  170\nFinal Reward:  -370.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  171\nFinal Reward:  18.0\n-> Episode  172\nFinal Reward:  -119.0\n-> Episode  173\nFinal Reward:  -67.0\n-> Episode  174\nFinal Reward:  71.0\n-> Episode  175\nFinal Reward:  95.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  176\nFinal Reward:  95.0\n-> Episode  177\nFinal Reward:  76.0\n-> Episode  178\nFinal Reward:  -195.0\n-> Episode  179\nFinal Reward:  -38.0\n-> Episode  180\nFinal Reward:  83.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  181\nFinal Reward:  95.0\n-> Episode  182\nFinal Reward:  -345.0\n-> Episode  183\nFinal Reward:  95.0\n-> Episode  184\nFinal Reward:  57.0\n-> Episode  185\nFinal Reward:  -350.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  186\nFinal Reward:  83.0\n-> Episode  187\nFinal Reward:  83.0\n-> Episode  188\nFinal Reward:  71.0\n-> Episode  189\nFinal Reward:  -330.0\n-> Episode  190\nFinal Reward:  -107.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  191\nFinal Reward:  -360.0\n-> Episode  192\nFinal Reward:  76.0\n-> Episode  193\nFinal Reward:  -235.0\n-> Episode  194\nFinal Reward:  -365.0\n-> Episode  195\nFinal Reward:  3.0\nINFO:tensorflow:Assets written to: model/assets\n-> Episode  196\nFinal Reward:  76.0\n-> Episode  197\nFinal Reward:  37.0\n-> Episode  198\nFinal Reward:  95.0\n-> Episode  199\nFinal Reward:  13.0\nResults: mean: -68.4±173.4, min: -410.0, max: 95.0,\n"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "MAX_STEPS = 100\n",
    "\n",
    "game, actions = configure_game_training()\n",
    "agent = DQNAgent(len(actions),use_latest=False)\n",
    "\n",
    "scores = []\n",
    "\n",
    "print(\"---------------Starting training Doom Ai--------------\")\n",
    "game.init()\n",
    "for epoch in range(EPOCHS):\n",
    "    epsilon = EPSILON\n",
    "    print(\"-> Episode \",epoch)\n",
    "    game.new_episode()\n",
    "    \n",
    "    # First state to predict on as a starting point\n",
    "    s1 = preprocess(game.get_state().screen_buffer)\n",
    "    s_t_deque = deque(maxlen=4)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "\n",
    "    s_t = np.stack(s_t_deque, axis=0)\n",
    "    step = 0\n",
    "    while (not game.is_episode_finished()) or step < MAX_STEPS:\n",
    "        q_t = agent.get_qs(s_t) \n",
    "\n",
    "        # Decide if greedy or not\n",
    "        if random.random() <= epsilon:\n",
    "            epsilon -= EPSILON_DISCOUNT\n",
    "            a = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            a = np.argmax(q_t)\n",
    "\n",
    "        # TODO work on epsilon degredation techniques\n",
    "\n",
    "\n",
    "        # Execute action    \n",
    "        reward = game.make_action(actions[a],12) #frame repeat ?\n",
    "        isterminal = game.is_episode_finished()\n",
    "        s2 = preprocess(game.get_state().screen_buffer) if not isterminal else np.zeros((resolution))\n",
    "        s_t_deque.append(s2)\n",
    "        s_t2 = np.stack(s_t_deque, axis=0)\n",
    "\n",
    "        agent.update_replay_memory([s_t, a, reward, s_t2, isterminal])\n",
    "\n",
    "        # Update current input of states\n",
    "        s_t = s_t2\n",
    "        agent.train()\n",
    "        step += 1\n",
    "\n",
    "    if game.is_episode_finished():\n",
    "        final_reward = game.get_total_reward()\n",
    "        print(\"Final Reward: \", final_reward)\n",
    "        scores.append(final_reward)\n",
    "    \n",
    "    if (epoch % 5) == 0:\n",
    "        agent.save_model()\n",
    "    \n",
    "\n",
    "train_scores = np.array(scores)\n",
    "\n",
    "print(\"Results: mean: %.1f±%.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "         \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "======================================\nTraining finished. It's time to watch!\nTotal score:  -12.0\nTotal score:  -17.0\nTotal score:  -12.0\nTotal score:  -17.0\nTotal score:  -17.0\n"
    }
   ],
   "source": [
    "print(\"======================================\")\n",
    "print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "# Reinitialize the game with window visible\n",
    "game, _ = configure_game_training()\n",
    "game.set_window_visible(True)\n",
    "game.set_mode(vizdoom.Mode.ASYNC_PLAYER)\n",
    "game.init()\n",
    "\n",
    "for _ in range(5):\n",
    "    game.new_episode()\n",
    "\n",
    "    s1 = preprocess(game.get_state().screen_buffer)\n",
    "    s_t_deque = deque(maxlen=4)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "    s_t_deque.append(s1)\n",
    "\n",
    "    s_t = np.stack(s_t_deque, axis=2)\n",
    "    while not game.is_episode_finished():\n",
    "        q_t = agent.get_qs(s_t) \n",
    "        a = np.argmax(q_t)\n",
    "\n",
    "        # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "        game.set_action(actions[a])\n",
    "        for _ in range(12):\n",
    "            game.advance_action()\n",
    "\n",
    "        s2 = []\n",
    "        if not game.is_episode_finished:\n",
    "            s2 = preprocess(game.get_state().screen_buffer)\n",
    "        else:\n",
    "            break\n",
    "        s_t_deque.append(s2)\n",
    "        s_t2 = np.stack(s_t_deque, axis=2)\n",
    "\n",
    "    # Sleep between episodes\n",
    "    time.sleep(1.0)\n",
    "    score = game.get_total_reward()\n",
    "    print(\"Total score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitce553f8d58a746c5874dcf1bfcd11db3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}